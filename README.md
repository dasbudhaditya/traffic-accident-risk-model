# ðŸš— Prediction of Casualty Severity in Road Traffic Accidents

This project builds and compares several machine learning models to predict **casualty severity** in UK road traffic accidents (slight, serious, fatal), supporting an insurance use-case for **Aviva PLC**.   

---

## ðŸŽ¯ Business Context

Aviva wants to design **risk-sensitive car insurance pricing** based on predicted casualty severity. By predicting whether a casualty is likely to be slight, serious, or fatal, this model can support:

- **Risk-based pricing** of insurance policies  
- **Fraud detection**  
- **Customer segmentation** and targeted interventions  
- Prioritisation of **road safety measures**   

---

## ðŸ“Š Data

Prepared inputs from previous group work:   

- Cleaned and merged accident / casualty / vehicle tables  
- Imbalanced target variable:

  - Class 3 (slight): 83.3%  
  - Class 2 (serious): 15.6%  
  - Class 1 (fatal): 1.1%  

- Exported as:
  - `Xtrain.xlsx`, `Xtest.xlsx` (63 features)  
  - `ytrain.xlsx`, `ytest.xlsx` (casualty_severity)

---

## ðŸ§¹ Handling Class Imbalance

- Checked class distribution and confirmed **severe imbalance**.   
- Applied **SMOTE oversampling** on the training set to balance classes:

  - Each class oversampled to **9,282 observations**.  

This helps prevent the model from ignoring minority classes (fatal and serious casualties).

---

## ðŸ§  Models

Four models are built and compared:

1. **Baseline Decision Tree** (shallow depth = 3)   
2. **Random Forest (Model 1)** â€“ tuned via **RandomizedSearchCV**  
3. **Logistic Regression (Model 2)** â€“ tuned via **RandomizedSearchCV**  
4. **Decision Tree (Model 3)** â€“ tuned via **RandomizedSearchCV**  
5. **Random Forest (Model 4)** â€“ tuned via **GridSearchCV**   

Key evaluation metrics (macro-averaged):

- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**

---

## ðŸ“ˆ Key Results

From the comparison table:   

| Model                | Accuracy | Precision | Recall | F1-Score |
|----------------------|----------|-----------|--------|----------|
| Random Forest (Rand) | ~0.7947  | ~0.5174   | ~0.4249| ~0.4502  |
| Decision Tree (tuned)| ~0.7140  | ~0.3902   | ~0.4139| ~0.3979  |
| Baseline DT          | ~0.7627  | ~0.2812   | ~0.3518| ~0.2988  |
| Logistic Regression  | ~0.3953  | ~0.3471   | ~0.4022| ~0.2672  |

- **Random Forest (RandomizedSearchCV)** is selected as the **best model**.  
- GridSearchCV RF gave similar but slightly worse overall metrics.   

---

## ðŸ“Š Visualisations

- Confusion matrices for:
  - Baseline DT
  - Tuned RF
  - Tuned Logistic Regression
  - Tuned Decision Tree  
- Bar plots comparing metrics across models.   

---

## ðŸ›  Tech Stack

- Python  
- Pandas, NumPy  
- Scikit-Learn (RandomForestClassifier, DecisionTreeClassifier, LogisticRegression, GridSearchCV, RandomizedSearchCV)  
- Imbalanced-Learn (SMOTE)  
- Matplotlib, Seaborn  

---

## ðŸ“¬ Contact

Questions about the project or dataset?  
ðŸ“§ **budhaditydasfall@gmail.com**  
ðŸ”— [LinkedIn](https://www.linkedin.com/in/dasbudhaditya/)
